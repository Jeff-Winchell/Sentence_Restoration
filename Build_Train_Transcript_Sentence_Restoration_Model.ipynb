{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "183cfbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_Units=256\n",
    "Max_Allowed_Sequence_Length=512\n",
    "Target_Label_Weights=[0,.5/7.8,.5/1] #first element is for padding, 2nd for normal token, 3rd for sentence ending token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fc82343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.8 in target label weighting above denoting prevalence of normal tokens vs sentence ending token comes from the following SQL.\n",
    "# However, this counts words, not tokens. \n",
    "# And it doesn't handle abbreviations with periods or repetitive sentences enders like !!! or ??\n",
    "#\n",
    "#Select Avg(Words/Avg_Sentences) As Average_Words_Per_Sentence \n",
    "#\tFrom (select Document_Id,Sum(Occurrence_Count) As Words \n",
    "#\t\t\tFrom sys.dm_fts_index_keywords_by_document(db_id('TrainingCorpus'),object_id('TedTalk')) Word_Stat\n",
    "#\t\t\tWhere Word_Stat.column_id=(Select Columns.Ordinal_Position \n",
    "#\t\t\t\t\t\t\t\t\t\tFrom Information_Schema.COLUMNS \n",
    "#\t\t\t\t\t\t\t\t\t\tWhere TABLE_NAME='TedTalk' And COLUMN_NAME='Transcript')\n",
    "#\t\t\tGroup By Document_Id) Foo,\n",
    "#\t\t(Select Avg(1.0*Len(Transcript)-Len(Replace(Replace(Replace(Transcript,'!',''),'?',''),'.',''))) As Avg_Sentences \n",
    "#\t\t\tFrom TedTalk) Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "483f7cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Corpus_Transcripts():\n",
    "    import pyodbc\n",
    "    db = pyodbc.connect(r'Driver={SQL Server};Server=(local);Database=TrainingCorpus;Trusted_Connection=yes;',autocommit=True)\n",
    "    dblink=db.cursor()\n",
    "    Transcripts_Rows=dblink.execute(\"Select Transcript_lower_punc_only_sentence_boundary,TextIdxKey \\\n",
    "                                        From TedTalk \\\n",
    "                                        Where Transcript_lower_punc_only_sentence_boundary is not null\").fetchall()\n",
    "    dblink.close()\n",
    "    db.close()\n",
    "    Transcripts=dict()\n",
    "    for row in Transcripts_Rows:\n",
    "        Transcripts[row[1]]=row[0]\n",
    "    return(Transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44bfa0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transcripts=Get_Corpus_Transcripts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d43b34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Corpus_Dictionary(Transcripts):\n",
    "\n",
    "    #Transcripts assumed to be list of lists of spoken words with only punctuation \n",
    "    #          being sentence ending punctuation, all of which are space separated \n",
    "    import pickle, os\n",
    "    if not os.path.exists('Corpus_Token.pickle'):\n",
    "        import collections,nltk\n",
    "        Corpus_Tokenizer = nltk.tokenize.treebank.TreebankWordTokenizer()\n",
    "        Corpus_Token_Frequency = collections.Counter()\n",
    "        for Transcript in Transcripts.values():\n",
    "            for Token in Corpus_Tokenizer.tokenize(Transcript):\n",
    "                Corpus_Token_Frequency[Token.lower()] += 1\n",
    "                \n",
    "        Corpus_Token_Index=dict({0:'<PAD>',1:'<OOV>'})\n",
    "        Token_Number=2 # most frequent word/token\n",
    "        for Token in collections.OrderedDict(Corpus_Token_Frequency.most_common()).keys():\n",
    "            Corpus_Token_Index[Token]=Token_Number\n",
    "            Idx+=1\n",
    "        with open('Corpus_Token.pickle','wb') as Save_Corpus_Token:\n",
    "            pickle.dump(Corpus_Token_Index,Save_Corpus_Token)\n",
    "    else:\n",
    "        Corpus_Token_Index=pickle.load(open('Corpus_Token.pickle','rb'))\n",
    "    return(Corpus_Token_Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebda5073",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus_Token_Index=Build_Corpus_Dictionary(Transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20123fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_GloVe_Encoded_Corpus(Corpus_Token_Index,Transcripts):\n",
    "    import numpy,os,pickle\n",
    "    #Associate Corpus token with GloVe vector\n",
    "    if not os.path.exists('glove300.pickle'):\n",
    "        with open('glove.42B.300d.txt', encoding=\"utf8\") as GloVe:  #uncased\n",
    "            GloVe_Corpus_Index = numpy.zeros(shape=(len(Corpus_Token_Index)+2,300),dtype=numpy.float32)\n",
    "            for Token_Vector in GloVe:\n",
    "                Token, *Vector = Token_Vector.split()\n",
    "                if Token in Corpus_Token_Index:\n",
    "                    GloVe_Corpus_Index[Corpus_Token_Index[Token]]=numpy.array(vector[-300:], dtype=numpy.float32)\n",
    "            GloVe_Corpus_Index[1]=numpy.mean(GloVe_Corpus_Index[2:,],axis=0) #OOV gets average GloVe vector\n",
    "        with open('glove300.pickle','wb') as Save_Glove:\n",
    "            pickle.dump(GloVe_Corpus_Index,Save_Glove)\n",
    "    else:\n",
    "        GloVe_Corpus_Index=pickle.load(open('glove300.pickle','rb'))\n",
    "\n",
    "    #Set all words in Corpus_Token_Index that aren't in GloVe to index 1 (OOV) so they can be given the average GloVe Vector\n",
    "    for Token,Token_Number in Corpus_Token_Index.items():\n",
    "        if numpy.sum(GloVe_Corpus_Index[Token_Number,:])==0:  ##Corpus_Token_Not_In_GloVe\n",
    "            Corpus_Token_Index[Token]=1\n",
    "    return(Corpus_Token_Index,GloVe_Corpus_Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84267c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus_Token_Index,GloVe_Corpus_Index=Build_GloVe_Encoded_Corpus(Corpus_Token_Index,Transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d877298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training_Text_To_Sequences(Transcripts,Corpus_Token_Index,Max_Allowed_Sequence_Length):\n",
    "    import nltk,numpy\n",
    "    Tokenizer = nltk.tokenize.treebank.TreebankWordTokenizer()\n",
    "    Transcripts_Labels_Tokens=dict()\n",
    "    Longest_Sequence=0\n",
    "    for Transcript_Id,Transcript in Transcripts.items():\n",
    "        Labels=[];Tokens=[];Sequence_Index=0;\n",
    "        Transcript_Subset_Id=0\n",
    "        for Token in Tokenizer.tokenize(Transcript):\n",
    "            if any(Character in Token for Character in ['.','?','!']) and Sequence_Index>0:\n",
    "                if Sequence_Index-1<0 or Sequence_Index-1>=len(Labels):\n",
    "                    print(Transcript_Id,Transcript_Subset_Id,Sequence_Index,Tokens,Labels)\n",
    "                Labels[Sequence_Index-1]=2  # also should cover situation where sentence ends with \n",
    "                                            # multiple sentence ending tokens (e.g !?!?!?)\n",
    "            else:\n",
    "                if Sequence_Index==Max_Allowed_Sequence_Length: # output this portion of the transcript \n",
    "                                                                # and prepare for next transcript portion\n",
    "                    Longest_Sequence==Max_Allowed_Sequence_Length\n",
    "                    Transcripts_Labels_Tokens[Transcript_Id,Transcript_Subset_Id]=(Labels,Tokens)\n",
    "                    Labels=[];Tokens=[];Sequence_Index=0;\n",
    "                    Transcript_Subset_Id+=1\n",
    "                Tokens.append(Corpus_Token_Index[Token.lower()] if Token.lower() in Corpus_Token_Index else 1) #Handle OOV token\n",
    "                Labels.append(1)\n",
    "                \n",
    "                Sequence_Index+=1\n",
    "        if Longest_Sequence!=Max_Allowed_Sequence_Length:\n",
    "            Longest_Sequence=len(Labels)\n",
    "        Transcripts_Labels_Tokens[Transcript_Id,Transcript_Subset_Id]=(Labels,Tokens)\n",
    "    \n",
    "    Padded_Transcripts_Labels=numpy.array([Labels+[0]*(Longest_Sequence-len(Labels)) \n",
    "                                           for Labels in [Label_Token[0] \n",
    "                                                          for Label_Token in Transcripts_Labels_Tokens.values()]])\n",
    "    Padded_Transcripts_Integers=numpy.array([Tokens+[0]*(Longest_Sequence-len(Tokens)) \n",
    "                                           for Tokens in [Label_Token[1] \n",
    "                                                          for Label_Token in Transcripts_Labels_Tokens.values()]])\n",
    "    \n",
    "    return(Padded_Transcripts_Labels,Padded_Transcripts_Integers,Longest_Sequence,Transcripts_Labels_Tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66178c8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Transcripts_Labels_Array,Transcripts_Integers_Array,Longest_Sequence,_ = Training_Text_To_Sequences(\n",
    "    Transcripts,\n",
    "    Corpus_Token_Index,\n",
    "    Max_Allowed_Sequence_Length)\n",
    "#following is needed by prediction program\n",
    "import pickle\n",
    "with open('Sentence_Restoration_Variables.pickle','wb') as Sentence_Restoration_Variables:\n",
    "    pickle.dump([LSTM_Units,Longest_Sequence,Target_Label_Weights],Sentence_Restoration_Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0223e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "#almost entirely (but rewritten for clarity) from https://stackoverflow.com/a/71265729/6147425    \n",
    "def Weighted_Loss(Target_Label_Weights):\n",
    "    def innerLoss(Actual, Predicted):\n",
    "        import tensorflow\n",
    "        return tensorflow.reshape(tensorflow.gather(Target_Label_Weights, Actual), (-1,Longest_Sequence)) \\\n",
    "               * \\\n",
    "         tensorflow.keras.losses.SparseCategoricalCrossentropy(reduction=tensorflow.keras.losses.Reduction.NONE)(Actual,Predicted)\n",
    "    return innerLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cf1f596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Model(LSTM_Units,Longest_Sequence,GloVe_Corpus_Index,Corpus_Token_Index):\n",
    "    import tensorflow\n",
    "    Model=tensorflow.keras.models.Sequential(name='BiLSTM_GloVe_Model')\n",
    "    Model.add(tensorflow.keras.Input(shape=(Longest_Sequence,), dtype='int32',name='Input'))\n",
    "    Model.add(tensorflow.keras.layers.Embedding(input_dim=len(Corpus_Token_Index) + 2,\n",
    "                                      output_dim=300,\n",
    "                                      embeddings_initializer=tensorflow.keras.initializers.Constant(GloVe_Corpus_Index),\n",
    "                                      input_length=Longest_Sequence,\n",
    "                                      mask_zero=True,\n",
    "                                      name='GloVe_300_Dim',\n",
    "                                      trainable=False))\n",
    "    Model.add(tensorflow.keras.layers.Bidirectional(layer=tensorflow.keras.layers.LSTM(units=LSTM_Units,\n",
    "                                                                                       return_sequences=True,\n",
    "                                                                                       activation=\"tanh\",\n",
    "                                                                                       recurrent_activation=\"sigmoid\",\n",
    "                                                                                       recurrent_dropout=0.0,\n",
    "                                                                                       unroll=False,\n",
    "                                                                                       use_bias=True\n",
    "                                                                                      )\n",
    "                                                    ,name='LSTM_'+str(LSTM_Units)+'_Seq_1'))\n",
    "    Model.add(tensorflow.keras.layers.Bidirectional(layer=tensorflow.keras.layers.LSTM(units=LSTM_Units,\n",
    "                                                                                       return_sequences=True,\n",
    "                                                                                       activation=\"tanh\",\n",
    "                                                                                       recurrent_activation=\"sigmoid\",\n",
    "                                                                                       recurrent_dropout=0.0,\n",
    "                                                                                       unroll=False,\n",
    "                                                                                       use_bias=True\n",
    "                                                                                      )\n",
    "                                                    ,name='LSTM_'+str(LSTM_Units)+'_Seq_2'))\n",
    "    Model.add(tensorflow.keras.layers.Bidirectional(layer=tensorflow.keras.layers.LSTM(units=LSTM_Units,\n",
    "                                                                                       return_sequences=True,\n",
    "                                                                                       activation=\"tanh\",\n",
    "                                                                                       recurrent_activation=\"sigmoid\",\n",
    "                                                                                       recurrent_dropout=0.0,\n",
    "                                                                                       unroll=False,\n",
    "                                                                                       use_bias=True\n",
    "                                                                                      )\n",
    "                                                    ,name='LSTM_'+str(LSTM_Units)+'_Seq_3'))\n",
    "    Model.add(tensorflow.keras.layers.Dropout(rate=.1,name='Dropout_.1'))\n",
    "    Model.add(tensorflow.keras.layers.Dense(units=3,\n",
    "                                                                kernel_initializer='normal',\n",
    "                                                                activation='sigmoid',\n",
    "                                                                name='Dense'))\n",
    "    Model.compile(loss=Weighted_Loss(Target_Label_Weights=[0,.5/7.8,.5/1]),optimizer='adam')\n",
    "    Model.save_weights('Temp_Save_Weights.keras')\n",
    "    print(Model.summary())\n",
    "    return Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b34de32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"BiLSTM_GloVe_Model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " GloVe_300_Dim (Embedding)   (None, 512, 300)          21021600  \n",
      "                                                                 \n",
      " LSTM_256_Seq_1 (Bidirection  (None, 512, 512)         1140736   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " LSTM_256_Seq_2 (Bidirection  (None, 512, 512)         1574912   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " LSTM_256_Seq_3 (Bidirection  (None, 512, 512)         1574912   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " Dropout_.1 (Dropout)        (None, 512, 512)          0         \n",
      "                                                                 \n",
      " Dense (Dense)               (None, 512, 3)            1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,313,699\n",
      "Trainable params: 4,292,099\n",
      "Non-trainable params: 21,021,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "Model=Build_Model(LSTM_Units,Longest_Sequence,GloVe_Corpus_Index,Corpus_Token_Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38c3941f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Train_Model(Model,LSTM_Units,Transcripts_Integers_Array,Transcripts_Labels_Array,Longest_Sequence):\n",
    "    import sklearn,statistics,numpy,itertools,math,tensorflow\n",
    "    Best_Epochs_for_each_split = list()\n",
    "    F1_for_each_split = list()\n",
    "    for Cross_Validation_Iteration,(train_index, test_index) in enumerate(\n",
    "                sklearn.model_selection.KFold(n_splits=3,shuffle = True,random_state=42\n",
    "                    ).split(Transcripts_Integers_Array,Transcripts_Labels_Array)):\n",
    "        print('Iteration',Cross_Validation_Iteration+1,'of 3')\n",
    "        Model.load_weights('Temp_Save_Weights.keras')\n",
    "        Training_History=Model.fit(x=Transcripts_Integers_Array[train_index],\n",
    "                           y=Transcripts_Labels_Array[train_index],\n",
    "                           validation_data=(Transcripts_Integers_Array[test_index], Transcripts_Labels_Array[test_index]),\n",
    "                           verbose=2,\n",
    "                           epochs=40, #actual epochs may be reduced by EarlyStopping\n",
    "                           steps_per_epoch = len(Transcripts_Labels_Array[train_index]) // 8,\n",
    "                           validation_steps = len(Transcripts_Labels_Array[test_index]) // 8,\n",
    "                           batch_size=8,  \n",
    "                           callbacks=[tensorflow.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                                              min_delta=0.0001,\n",
    "                                                                              patience=2,\n",
    "                                                                              verbose=1,\n",
    "                                                                              mode=\"min\",\n",
    "                                                                              restore_best_weights=False),\n",
    "                                     tensorflow.keras.callbacks.ModelCheckpoint(\n",
    "                                         filepath=\"Restore_Sentence_\"+str(LSTM_Units)+\"unit_Triple_BiLSTM_\"\\\n",
    "                                             +str(Longest_Sequence)+\"MaxToken_KFold_\"+str(Cross_Validation_Iteration+1)+\".keras\",\n",
    "                                         monitor='val_loss',\n",
    "                                         save_weights_only=True,\n",
    "                                         verbose=1,\n",
    "                                         options = tensorflow.train.CheckpointOptions(experimental_enable_async_checkpoint=True),\n",
    "                                         save_best_only=True,\n",
    "                                         mode='min')])\n",
    "        print('Model Fit Done')\n",
    "\n",
    "        Best_Epochs_for_each_split.append(1+float(numpy.argmin(Training_History.history['val_loss'])))\n",
    "\n",
    "        Predicted_Classifications = numpy.argmax(Model.predict(x=Transcripts_Integers_Array[test_index]), axis=-1)\n",
    "        Predicted_Classifications_With_Padding_Info=list(zip(list(itertools.chain(*Predicted_Classifications.tolist())),\n",
    "                                                             list(itertools.chain(*Transcripts_Integers_Array[test_index].tolist()))))\n",
    "        True_Classifications_With_Padding_Info=list(zip(list(itertools.chain(*Transcripts_Labels_Array[test_index].tolist())),\n",
    "                                                        list(itertools.chain(*Transcripts_Integers_Array[test_index].tolist()))))\n",
    "\n",
    "        #Model may predict a non-pad is a pad, rare, but it happens so manually correct that until a better way is found\n",
    "        F1=sklearn.metrics.f1_score(y_true=[Token_Label[0] \n",
    "                                                for Token_Label in True_Classifications_With_Padding_Info \n",
    "                                                                if Token_Label[1]!=0],\n",
    "                                    y_pred=[Token_Label[0] if Token_Label[0]!=0 else 1 \n",
    "                                                for Token_Label in Predicted_Classifications_With_Padding_Info \n",
    "                                                                if Token_Label[1]!=0])\n",
    "        print(F1)\n",
    "        F1_for_each_split.append(F1)\n",
    "\n",
    "\n",
    "    print(F1_for_each_split)\n",
    "    #Assuming F1 for each kfold split is similar take the epoch number from the best one, tr\n",
    "    # and compute final fit model using all data\n",
    "    Model.load_weights('Temp_Save_Weights.keras')\n",
    "    Model.fit(x=Transcripts_Integers_Array,\n",
    "              y=Transcripts_Labels_Array,\n",
    "              epochs=math.ceil(statistics.mean(Best_Epochs_for_each_split)),\n",
    "              batch_size=8,\n",
    "              verbose=2,\n",
    "              steps_per_epoch = len(Transcripts_Labels_Array) // 8\n",
    "             )\n",
    "    Model.save('Restore_Sentence_'+str(LSTM_Units)+'_unit_Triple_BiLSTM_'+str(Longest_Sequence)+'MaxToken.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c93778a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 of 3\n",
      "Epoch 1/40\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.01601, saving model to Restore_Sentence_256unit_Triple_BiLSTM_512MaxToken_KFold_1.keras\n",
      "1349/1349 - 307s - loss: 0.0212 - val_loss: 0.0160 - 307s/epoch - 228ms/step\n",
      "Epoch 2/40\n",
      "\n",
      "Epoch 2: val_loss improved from 0.01601 to 0.01414, saving model to Restore_Sentence_256unit_Triple_BiLSTM_512MaxToken_KFold_1.keras\n",
      "1349/1349 - 325s - loss: 0.0143 - val_loss: 0.0141 - 325s/epoch - 241ms/step\n",
      "Epoch 3/40\n",
      "\n",
      "Epoch 3: val_loss improved from 0.01414 to 0.01353, saving model to Restore_Sentence_256unit_Triple_BiLSTM_512MaxToken_KFold_1.keras\n",
      "1349/1349 - 1591s - loss: 0.0120 - val_loss: 0.0135 - 1591s/epoch - 1s/step\n",
      "Epoch 4/40\n",
      "\n",
      "Epoch 4: val_loss improved from 0.01353 to 0.01329, saving model to Restore_Sentence_256unit_Triple_BiLSTM_512MaxToken_KFold_1.keras\n",
      "1349/1349 - 9674s - loss: 0.0101 - val_loss: 0.0133 - 9674s/epoch - 7s/step\n",
      "Epoch 5/40\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.01329\n",
      "1349/1349 - 449s - loss: 0.0085 - val_loss: 0.0147 - 449s/epoch - 333ms/step\n",
      "Epoch 6/40\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.01329\n",
      "1349/1349 - 465s - loss: 0.0071 - val_loss: 0.0164 - 465s/epoch - 345ms/step\n",
      "Epoch 6: early stopping\n",
      "Model Fit Done\n",
      "169/169 [==============================] - 30s 140ms/step\n",
      "0.9744050564988234\n",
      "Iteration 2 of 3\n",
      "Epoch 1/40\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.01723, saving model to Restore_Sentence_256unit_Triple_BiLSTM_512MaxToken_KFold_2.keras\n",
      "1349/1349 - 441s - loss: 0.0239 - val_loss: 0.0172 - 441s/epoch - 327ms/step\n",
      "Epoch 2/40\n",
      "\n",
      "Epoch 2: val_loss improved from 0.01723 to 0.01517, saving model to Restore_Sentence_256unit_Triple_BiLSTM_512MaxToken_KFold_2.keras\n",
      "1349/1349 - 551s - loss: 0.0159 - val_loss: 0.0152 - 551s/epoch - 408ms/step\n",
      "Epoch 3/40\n",
      "\n",
      "Epoch 3: val_loss improved from 0.01517 to 0.01456, saving model to Restore_Sentence_256unit_Triple_BiLSTM_512MaxToken_KFold_2.keras\n",
      "1349/1349 - 525s - loss: 0.0138 - val_loss: 0.0146 - 525s/epoch - 389ms/step\n",
      "Epoch 4/40\n",
      "\n",
      "Epoch 4: val_loss improved from 0.01456 to 0.01369, saving model to Restore_Sentence_256unit_Triple_BiLSTM_512MaxToken_KFold_2.keras\n",
      "1349/1349 - 516s - loss: 0.0122 - val_loss: 0.0137 - 516s/epoch - 382ms/step\n",
      "Epoch 5/40\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.01369\n",
      "1349/1349 - 603s - loss: 0.0107 - val_loss: 0.0142 - 603s/epoch - 447ms/step\n",
      "Epoch 6/40\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.01369\n",
      "1349/1349 - 557s - loss: 0.0094 - val_loss: 0.0152 - 557s/epoch - 413ms/step\n",
      "Epoch 6: early stopping\n",
      "Model Fit Done\n",
      "169/169 [==============================] - 27s 162ms/step\n",
      "0.969890797689428\n",
      "Iteration 3 of 3\n",
      "Epoch 1/40\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.01727, saving model to Restore_Sentence_256unit_Triple_BiLSTM_512MaxToken_KFold_3.keras\n",
      "1349/1349 - 537s - loss: 0.0231 - val_loss: 0.0173 - 537s/epoch - 398ms/step\n",
      "Epoch 2/40\n",
      "\n",
      "Epoch 2: val_loss improved from 0.01727 to 0.01531, saving model to Restore_Sentence_256unit_Triple_BiLSTM_512MaxToken_KFold_3.keras\n",
      "1349/1349 - 539s - loss: 0.0157 - val_loss: 0.0153 - 539s/epoch - 400ms/step\n",
      "Epoch 3/40\n",
      "\n",
      "Epoch 3: val_loss improved from 0.01531 to 0.01437, saving model to Restore_Sentence_256unit_Triple_BiLSTM_512MaxToken_KFold_3.keras\n",
      "1349/1349 - 546s - loss: 0.0137 - val_loss: 0.0144 - 546s/epoch - 405ms/step\n",
      "Epoch 4/40\n",
      "\n",
      "Epoch 4: val_loss improved from 0.01437 to 0.01437, saving model to Restore_Sentence_256unit_Triple_BiLSTM_512MaxToken_KFold_3.keras\n",
      "1349/1349 - 495s - loss: 0.0121 - val_loss: 0.0144 - 495s/epoch - 367ms/step\n",
      "Epoch 5/40\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.01437\n",
      "1349/1349 - 475s - loss: 0.0107 - val_loss: 0.0150 - 475s/epoch - 352ms/step\n",
      "Epoch 5: early stopping\n",
      "Model Fit Done\n",
      "169/169 [==============================] - 24s 143ms/step\n",
      "0.9733472121586362\n",
      "[0.9744050564988234, 0.969890797689428, 0.9733472121586362]\n",
      "Epoch 1/4\n",
      "2024/2024 - 563s - loss: 0.0208 - 563s/epoch - 278ms/step\n",
      "Epoch 2/4\n",
      "2024/2024 - 559s - loss: 0.0145 - 559s/epoch - 276ms/step\n",
      "Epoch 3/4\n",
      "2024/2024 - 563s - loss: 0.0126 - 563s/epoch - 278ms/step\n",
      "Epoch 4/4\n",
      "2024/2024 - 563s - loss: 0.0111 - 563s/epoch - 278ms/step\n"
     ]
    }
   ],
   "source": [
    "Train_Model(Model,LSTM_Units,Transcripts_Integers_Array,Transcripts_Labels_Array,Longest_Sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
