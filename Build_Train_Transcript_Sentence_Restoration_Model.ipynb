{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b7d6657",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_Units=256\n",
    "Max_Allowed_Sequence_Length=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "483f7cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Corpus_Transcripts():\n",
    "    import pyodbc\n",
    "    db = pyodbc.connect(r'Driver={SQL Server};Server=(local);Database=TrainingCorpus;Trusted_Connection=yes;',autocommit=True)\n",
    "    dblink=db.cursor()\n",
    "    Transcripts_Rows=dblink.execute(\"Select Transcript_lower_punc_only_sentence_boundary,TextIdxKey \\\n",
    "                                        From TedTalk \\\n",
    "                                        Where Transcript_lower_punc_only_sentence_boundary is not null\").fetchall()\n",
    "    dblink.close()\n",
    "    db.close()\n",
    "    Transcripts=dict()\n",
    "    for row in Transcripts_Rows:\n",
    "        Transcripts[row[1]]=row[0]\n",
    "    return(Transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44bfa0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transcripts=Get_Corpus_Transcripts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d43b34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Corpus_Dictionary(Transcripts):\n",
    "\n",
    "    #Transcripts assumed to be list of lists of spoken words with only punctuation \n",
    "    #          being sentence ending punctuation, all of which are space separated \n",
    "    import pickle, os\n",
    "    if not os.path.exists('Corpus_Token.pickle'):\n",
    "        import collections,nltk\n",
    "        Corpus_Tokenizer = nltk.tokenize.treebank.TreebankWordTokenizer()\n",
    "        Corpus_Token_Frequency = collections.Counter()\n",
    "        for Transcript in Transcripts.values():\n",
    "            for Token in Corpus_Tokenizer.tokenize(Transcript):\n",
    "                Corpus_Token_Frequency[Token.lower()] += 1\n",
    "                \n",
    "        Corpus_Token_Index=dict({0:'<PAD>',1:'<OOV>'})\n",
    "        Token_Number=2 # most frequent word/token\n",
    "        for Token in collections.OrderedDict(Corpus_Token_Frequency.most_common()).keys():\n",
    "            Corpus_Token_Index[Token]=Token_Number\n",
    "            Idx+=1\n",
    "        with open('Corpus_Token.pickle','wb') as Save_Corpus_Token:\n",
    "            pickle.dump(Corpus_Token_Index,Save_Corpus_Token)\n",
    "    else:\n",
    "        Corpus_Token_Index=pickle.load(open('Corpus_Token.pickle','rb'))\n",
    "    return(Corpus_Token_Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebda5073",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus_Token_Index=Build_Corpus_Dictionary(Transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20123fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_GloVe_Encoded_Corpus(Corpus_Token_Index,Transcripts):\n",
    "    import numpy,os,pickle\n",
    "    #Associate Corpus token with GloVe vector\n",
    "    if not os.path.exists('glove300.pickle'):\n",
    "        with open('glove.42B.300d.txt', encoding=\"utf8\") as GloVe:  #uncased\n",
    "            GloVe_Corpus_Index = numpy.zeros(shape=(len(Corpus_Token_Index)+2,300),dtype=numpy.float32)\n",
    "            for Token_Vector in GloVe:\n",
    "                Token, *Vector = Token_Vector.split()\n",
    "                if Token in Corpus_Token_Index:\n",
    "                    GloVe_Corpus_Index[Corpus_Token_Index[Token]]=numpy.array(vector[-300:], dtype=numpy.float32)\n",
    "            GloVe_Corpus_Index[1]=numpy.mean(GloVe_Corpus_Index[2:,],axis=0) #OOV gets average GloVe vector\n",
    "        with open('glove300.pickle','wb') as Save_Glove:\n",
    "            pickle.dump(GloVe_Corpus_Index,Save_Glove)\n",
    "    else:\n",
    "        GloVe_Corpus_Index=pickle.load(open('glove300.pickle','rb'))\n",
    "\n",
    "    #Set all words in Corpus_Token_Index that aren't in GloVe to index 1 (OOV) so they can be given the average GloVe Vector\n",
    "    for Token,Token_Number in Corpus_Token_Index.items():\n",
    "        if numpy.sum(GloVe_Corpus_Index[Token_Number,:])==0:  ##Corpus_Token_Not_In_GloVe\n",
    "            Corpus_Token_Index[Token]=1\n",
    "    return(Corpus_Token_Index,GloVe_Corpus_Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84267c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus_Token_Index,GloVe_Corpus_Index=Build_GloVe_Encoded_Corpus(Corpus_Token_Index,Transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d877298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training_Text_To_Sequences(Transcripts,Corpus_Token_Index,Max_Allowed_Sequence_Length):\n",
    "    import nltk,numpy\n",
    "    Tokenizer = nltk.tokenize.treebank.TreebankWordTokenizer()\n",
    "    Transcripts_Labels_Tokens=dict()\n",
    "    Longest_Sequence=0\n",
    "    for Transcript_Id,Transcript in Transcripts.items():\n",
    "        Labels=[];Tokens=[];Sequence_Index=0;\n",
    "        Transcript_Subset_Id=0\n",
    "        for Token in Tokenizer.tokenize(Transcript):\n",
    "            if any(Character in Token for Character in ['.','?','!']) and Sequence_Index>0:\n",
    "                if Sequence_Index-1<0 or Sequence_Index-1>=len(Labels):\n",
    "                    print(Transcript_Id,Transcript_Subset_Id,Sequence_Index,Tokens,Labels)\n",
    "                Labels[Sequence_Index-1]=2  # also should cover situation where sentence ends with \n",
    "                                            # multiple sentence ending tokens (e.g !?!?!?)\n",
    "            else:\n",
    "                if Sequence_Index==Max_Allowed_Sequence_Length: # output this portion of the transcript \n",
    "                                                                # and prepare for next transcript portion\n",
    "                    Longest_Sequence==Max_Allowed_Sequence_Length\n",
    "                    Transcripts_Labels_Tokens[Transcript_Id,Transcript_Subset_Id]=(Labels,Tokens)\n",
    "                    Labels=[];Tokens=[];Sequence_Index=0;\n",
    "                    Transcript_Subset_Id+=1\n",
    "                Tokens.append(Corpus_Token_Index[Token.lower()] if Token.lower() in Corpus_Token_Index else 1) #Handle OOV token\n",
    "                Labels.append(1)\n",
    "                \n",
    "                Sequence_Index+=1\n",
    "        if Longest_Sequence!=Max_Allowed_Sequence_Length:\n",
    "            Longest_Sequence=len(Labels)\n",
    "        Transcripts_Labels_Tokens[Transcript_Id,Transcript_Subset_Id]=(Labels,Tokens)\n",
    "    \n",
    "    Padded_Transcripts_Labels=numpy.array([Labels+[0]*(Longest_Sequence-len(Labels)) \n",
    "                                           for Labels in [Label_Token[0] \n",
    "                                                          for Label_Token in Transcripts_Labels_Tokens.values()]])\n",
    "    Padded_Transcripts_Integers=numpy.array([Tokens+[0]*(Longest_Sequence-len(Tokens)) \n",
    "                                           for Tokens in [Label_Token[1] \n",
    "                                                          for Label_Token in Transcripts_Labels_Tokens.values()]])\n",
    "    \n",
    "    return(Padded_Transcripts_Labels,Padded_Transcripts_Integers,Longest_Sequence,Transcripts_Labels_Tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66178c8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Transcripts_Labels_Array,Transcripts_Integers_Array,Longest_Sequence,_ = Training_Text_To_Sequences(\n",
    "    Transcripts,\n",
    "    Corpus_Token_Index,\n",
    "    Max_Allowed_Sequence_Length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cf1f596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Model(LSTM_Units,Longest_Sequence,GloVe_Corpus_Index,Corpus_Token_Index):\n",
    "    import tensorflow\n",
    "    Model=tensorflow.keras.models.Sequential(name='BiLSTM_GloVe_Model')\n",
    "    Model.add(tensorflow.keras.Input(shape=(Longest_Sequence,), dtype='int32',name='Input'))\n",
    "    Model.add(tensorflow.keras.layers.Embedding(input_dim=len(Corpus_Token_Index) + 2,\n",
    "                                      output_dim=300,\n",
    "                                      embeddings_initializer=tensorflow.keras.initializers.Constant(GloVe_Corpus_Index),\n",
    "                                      input_length=Longest_Sequence,\n",
    "                                      mask_zero=True,\n",
    "                                      name='GloVe_300_Dim',\n",
    "                                      trainable=False))\n",
    "    Model.add(tensorflow.keras.layers.Bidirectional(layer=tensorflow.keras.layers.LSTM(units=LSTM_Units,\n",
    "                                                                                       return_sequences=True,\n",
    "                                                                                       activation=\"tanh\",\n",
    "                                                                                       recurrent_activation=\"sigmoid\",\n",
    "                                                                                       recurrent_dropout=0.0,\n",
    "                                                                                       unroll=False,\n",
    "                                                                                       use_bias=True\n",
    "                                                                                      )\n",
    "                                                    ,name='LSTM_'+str(LSTM_Units)+'_Seq_1'))\n",
    "    Model.add(tensorflow.keras.layers.Bidirectional(layer=tensorflow.keras.layers.LSTM(units=LSTM_Units,\n",
    "                                                                                       return_sequences=True,\n",
    "                                                                                       activation=\"tanh\",\n",
    "                                                                                       recurrent_activation=\"sigmoid\",\n",
    "                                                                                       recurrent_dropout=0.0,\n",
    "                                                                                       unroll=False,\n",
    "                                                                                       use_bias=True\n",
    "                                                                                      )\n",
    "                                                    ,name='LSTM_'+str(LSTM_Units)+'_Seq_2'))\n",
    "    Model.add(tensorflow.keras.layers.Bidirectional(layer=tensorflow.keras.layers.LSTM(units=LSTM_Units,\n",
    "                                                                                       return_sequences=True,\n",
    "                                                                                       activation=\"tanh\",\n",
    "                                                                                       recurrent_activation=\"sigmoid\",\n",
    "                                                                                       recurrent_dropout=0.0,\n",
    "                                                                                       unroll=False,\n",
    "                                                                                       use_bias=True\n",
    "                                                                                      )\n",
    "                                                    ,name='LSTM_'+str(LSTM_Units)+'_Seq_3'))\n",
    "    Model.add(tensorflow.keras.layers.Dropout(rate=.1,name='Dropout_.1'))\n",
    "    Model.add(tensorflow.keras.layers.Dense(units=3,\n",
    "                                                                kernel_initializer='normal',\n",
    "                                                                activation='sigmoid',\n",
    "                                                                name='Dense'))\n",
    "    Model.compile(loss=tensorflow.keras.losses.SparseCategoricalCrossentropy(ignore_class=0,\n",
    "                from_logits=False, reduction=tensorflow.keras.losses.Reduction.NONE\n",
    "            ),optimizer='adam')\n",
    "    Model.save_weights('Temp_Save_Weights.keras')\n",
    "    print(Model.summary())\n",
    "    return Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "840bbe12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"BiLSTM_GloVe_Model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " GloVe_300_Dim (Embedding)   (None, 512, 300)          21021600  \n",
      "                                                                 \n",
      " LSTM_256_Seq_1 (Bidirection  (None, 512, 512)         1140736   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " LSTM_256_Seq_2 (Bidirection  (None, 512, 512)         1574912   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " LSTM_256_Seq_3 (Bidirection  (None, 512, 512)         1574912   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " Dropout_.1 (Dropout)        (None, 512, 512)          0         \n",
      "                                                                 \n",
      " Dense (Dense)               (None, 512, 3)            1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,313,699\n",
      "Trainable params: 4,292,099\n",
      "Non-trainable params: 21,021,600\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "Model=Build_Model(LSTM_Units,Longest_Sequence,GloVe_Corpus_Index,Corpus_Token_Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38c3941f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Train_Model(Model,LSTM_Units,Transcripts_Integers_Array,Transcripts_Labels_Array,Longest_Sequence):\n",
    "    import sklearn,statistics,numpy,itertools,math,tensorflow\n",
    "    Best_Epochs_for_each_split = list()\n",
    "    F1_for_each_split = list()\n",
    "    LSTM_Units=256\n",
    "    for Cross_Validation_Iteration,(train_index, test_index) in enumerate(\n",
    "                sklearn.model_selection.KFold(n_splits=5,shuffle = True,random_state=42\n",
    "                    ).split(Transcripts_Integers_Array,Transcripts_Labels_Array)):\n",
    "        print('Iteration',Cross_Validation_Iteration+1,'of 5')\n",
    "        Model.load_weights('Temp_Save_Weights.keras')\n",
    "        Training_History=Model.fit(x=Transcripts_Integers_Array[train_index],\n",
    "                           y=Transcripts_Labels_Array[train_index],\n",
    "                           validation_data=(Transcripts_Integers_Array[test_index], Transcripts_Labels_Array[test_index]),\n",
    "                           verbose=2,\n",
    "                           epochs=40, #actual epochs may be reduced by EarlyStopping\n",
    "                           steps_per_epoch = len(Transcripts_Labels_Array[train_index]) // 8,\n",
    "                           validation_steps = len(Transcripts_Labels_Array[test_index]) // 8,\n",
    "                           batch_size=8,  \n",
    "                           callbacks=[tensorflow.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                                              min_delta=0.0001,\n",
    "                                                                              patience=2,\n",
    "                                                                              verbose=1,\n",
    "                                                                              mode=\"min\",\n",
    "                                                                              restore_best_weights=False),\n",
    "                                     tensorflow.keras.callbacks.ModelCheckpoint(\n",
    "                                         filepath=\"Restore_Sentence_\"+str(LSTM_Units)+\"unit_Triple_BiLSTM_\"\\\n",
    "                                             +str(Longest_Sequence)+\"MaxToken_KFold_\"+str(Cross_Validation_Iteration+1)+\".keras\",\n",
    "                                         monitor='val_loss',\n",
    "                                         save_weights_only=True,\n",
    "                                         verbose=1,\n",
    "                                         options = tensorflow.train.CheckpointOptions(experimental_enable_async_checkpoint=True),\n",
    "                                         save_best_only=True,\n",
    "                                         mode='min')])\n",
    "        print('Model Fit Done')\n",
    "\n",
    "        Best_Epochs_for_each_split.append(1+float(numpy.argmin(Training_History.history['val_loss'])))\n",
    "\n",
    "        Predicted_Classifications = numpy.argmax(Model.predict(x=Transcripts_Integers_Array[test_index]), axis=-1)\n",
    "        Predicted_Classifications_With_Padding_Info=list(zip(list(itertools.chain(*Predicted_Classifications.tolist())),\n",
    "                                                             list(itertools.chain(*Transcripts_Integers_Array[test_index].tolist()))))\n",
    "        True_Classifications_With_Padding_Info=list(zip(list(itertools.chain(*Transcripts_Labels_Array[test_index].tolist())),\n",
    "                                                        list(itertools.chain(*Transcripts_Integers_Array[test_index].tolist()))))\n",
    "\n",
    "        #Model may predict a non-pad is a pad, rare, but it happens so manually correct that until a better way is found\n",
    "        F1=sklearn.metrics.f1_score(y_true=[Token_Label[0] \n",
    "                                                for Token_Label in True_Classifications_With_Padding_Info \n",
    "                                                                if Token_Label[1]!=0],\n",
    "                                    y_pred=[Token_Label[0] if Token_Label[0]!=0 else 1 \n",
    "                                                for Token_Label in Predicted_Classifications_With_Padding_Info \n",
    "                                                                if Token_Label[1]!=0])\n",
    "        print(F1)\n",
    "        F1_for_each_split.append(F1)\n",
    "\n",
    "\n",
    "    print(F1_for_each_split)\n",
    "    #Assuming F1 for each kfold split is similar take the epoch number from the best one, \n",
    "    # and compute final fit model using all data\n",
    "    Model.load_weights('Temp_Save_Weights.keras')\n",
    "    Model.fit(x=Transcripts_Integers_Array,\n",
    "              y=Transcripts_Labels_Array,\n",
    "              epochs=math.ceil(statistics.mean(Best_Epochs_for_each_split)),\n",
    "              batch_size=8,\n",
    "              verbose=2,\n",
    "              steps_per_epoch = len(Transcripts_Labels_Array) // 8\n",
    "             )\n",
    "    Model.save('Restore_Sentence_'+str(LSTM_Units)+'_unit_BiLSTM_'+str(Longest_Sequence)+'MaxToken.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b44b87",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Train_Model(Model,LSTM_Units,Transcripts_Integers_Array,Transcripts_Labels_Array,Longest_Sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
